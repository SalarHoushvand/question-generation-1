.A set is an unordered collection of different elements. A set can be written explicitly by listing its elements using set bracket. If the order of the elements is changed or any element of a set is repeated, it does not make any changes in the set. an apple is a fruit.Universal set contains any relevant elements and depends on application. An empty set is a subset of every set including itself.
The power set is a set of all subsets of a set. It includes the null set and also the set itself.
The power set is a set of sets.the elements of the power set are themselves sets.An ordered pair is simply a pair of objects. Ordered pairs are also called 2-tuples or sequences. 
Language modeling is the task of assigning a probability to sentences in a language.
Perfect performance at the language modeling task, namely predicting the next word in a
sequence with a number of guesses that is the same as or lower than the number of guesses required
by a human participant, is an indication of human level intelligence, and is unlikely to be achieved
in the near future.language modeling is a
crucial components in real-world applications such as machine-translation and automatic speech
recognition, where the system produces several translation or transcription hypotheses, which are
then scored by a language model. For these reasons, language modeling plays a central role in
natural-language processing, AI, and machine-learning research.The smoothing techniques are intricate and based on back off to lower-order events.this assumes a fixed backing-up order, that needs to be designed by hand, and makes it hard to add more "creative" conditioning contexts.The sequential nature of the backoff also makes it hard to scale toward larger ngrams in order to capture long-range dependencies.Neural networks provide a powerful learning machinery that is very appealing for use in natural language problems.provide a powerful learning machinery that is very appealing for use in natural language problems.A major component in neural networks for language is the use of an embedding layer.
When embedding words, they transform from being isolated distinct symbols into mathematical objects that can be operated on.
the representation of words as vectors is learned by the network as part of the training process.
the network also learns to combine word vectors in a way that is useful for
prediction.
There are two major kinds of neural network architectures, that can be combined in various
ways: feed-forward networks and recurrent/recursive networks.

When feeding the network with a set of input components, it learns to combine them in
a meaningful way. MLPs can be used whenever a linear model was previously used.

Convolutional feed-forward networks are specialized architectures that excel at extracting
local patterns in the data: they are fed arbitrarily sized inputs, and are capable of extracting meaningful
local patterns that are sensitive to word order, regardless of where they appear in the input.
Recurrent neural networks are specialized models for sequential data.

The sequence-to-sequence approach is very general.

the input is a base word and a
desired inflection request.

The conditioned-generation approach is very flexible.
The
 intuition is that different users have different communication styles, based on their age, gender,
social role, background knowledge, personality traits and many other latent factors.

the network can learn to adapt its predictions while
still using an underlying language model as a backbone.

the network also learns user embeddings.

The generator can then use these vectors as a read-only memory representing the conditioning
sentence.

